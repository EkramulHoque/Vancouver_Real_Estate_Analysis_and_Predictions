{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A-9aaABc-BVF"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt # rms = sqrt(mean_squared_error(y_true, y_predicted))\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# linear regression models\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet,BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# cross val, k-folds\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5618,
     "status": "ok",
     "timestamp": 1554342608899,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "fDjufxftCdqt",
    "outputId": "106f44cb-35f1-4f56-e9aa-df0723ff6c52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.regularizers import l1\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2h7qudI9DEMU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cz08MbRz-Bkt"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('FinalREW.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UzncnNvY-KqM"
   },
   "outputs": [],
   "source": [
    "df.drop('Unnamed: 0',axis=1,inplace=True)\n",
    "df.drop(df.columns[12:544],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nXU7Hdvb-KvZ"
   },
   "outputs": [],
   "source": [
    "X = df.drop(['price'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['price'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4DJgZReGqqk"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TGnTWHl-CJAE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1402478,
     "status": "ok",
     "timestamp": 1554346201863,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "qeFsWRITCJDi",
    "outputId": "7b413b31-c5fa-496e-800c-799eb86b0f54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support Vector Regression Scores- rmse: 734916.5834675301  r2: 0.4844043556316916\n"
     ]
    }
   ],
   "source": [
    "svr_model = SVR('linear',C=500)\n",
    "svr_model.fit(X_train,y_train)\n",
    "y_pred = svr_model.predict(X_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred)) \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Support Vector Regression Scores- rmse:\",rmse,\" r2:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-USuBlwnCJFA"
   },
   "outputs": [],
   "source": [
    "filename = 'svr_f1.pkl'\n",
    "pickle.dump(svr_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yxp4eDNZC8sl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9120,
     "status": "ok",
     "timestamp": 1554347321622,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "Slau2_KyCJJj",
    "outputId": "07ba660a-8912-4f25-f766-e7101fc96828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boost(No tuning) Regression Scores- rmse: 399439.27864794794  r2: 0.8476878029902623\n"
     ]
    }
   ],
   "source": [
    "params = {'n_estimators': 600, 'max_depth': 3,'max_features': 'auto',\n",
    "          'learning_rate': 0.1, 'loss': 'ls'}\n",
    "\n",
    "gbr_model = GradientBoostingRegressor(**params)\n",
    "gbr_model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = gbr_model.predict(X_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred)) \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Gradient Boost(No tuning) Regression Scores- rmse:\",rmse,\" r2:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GmMCRBAgCJNF"
   },
   "outputs": [],
   "source": [
    "filename = 'gbr_f2.pkl'\n",
    "pickle.dump(gbr_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rx-SfPEZC9cG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11966,
     "status": "ok",
     "timestamp": 1554347919662,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "2uWHdkeQCJOi",
    "outputId": "7403fd8a-b53c-4f24-ee77-c099c23acc7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Scores- rmse: 446583.2423091945  r2: 0.8096126933618631\n"
     ]
    }
   ],
   "source": [
    "rfr_model = RandomForestRegressor(max_depth=12, max_features='auto',min_samples_leaf=2, min_samples_split=10,n_estimators=300)\n",
    "rfr_model.fit(X_train,y_train)\n",
    "y_pred = rfr_model.predict(X_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred)) \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Random Forest Scores- rmse:\",rmse,\" r2:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "od6ljhrwCJIh"
   },
   "outputs": [],
   "source": [
    "filename = 'rfr_f3.pkl'\n",
    "pickle.dump(rfr_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcU9hA4lCI-Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10972
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 196721,
     "status": "ok",
     "timestamp": 1554348444992,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "pg53wQdE-Kyj",
    "outputId": "88fcafce-08eb-4c3d-edb8-a427a72fa6be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8624 samples, validate on 959 samples\n",
      "Epoch 1/300\n",
      "8624/8624 [==============================] - 1s 108us/step - loss: 1458245574671.1987 - val_loss: 565968037112.7925\n",
      "Epoch 2/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 511294809933.4174 - val_loss: 565345035769.0594\n",
      "Epoch 3/300\n",
      "8624/8624 [==============================] - 1s 78us/step - loss: 509169052229.3432 - val_loss: 564922346302.7321\n",
      "Epoch 4/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 508740889552.5046 - val_loss: 563408545170.5526\n",
      "Epoch 5/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 505995020246.2041 - val_loss: 560891729806.8154\n",
      "Epoch 6/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 504236972970.5084 - val_loss: 559859320953.7268\n",
      "Epoch 7/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 504024168626.5826 - val_loss: 557224992605.6976\n",
      "Epoch 8/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 501768907576.5195 - val_loss: 559948884252.0292\n",
      "Epoch 9/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 499493908381.2097 - val_loss: 553329383532.9135\n",
      "Epoch 10/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 496940979247.4954 - val_loss: 551613352536.0918\n",
      "Epoch 11/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 494446483756.1707 - val_loss: 549904726389.7227\n",
      "Epoch 12/300\n",
      "8624/8624 [==============================] - 1s 79us/step - loss: 492422790590.4564 - val_loss: 547544437543.2409\n",
      "Epoch 13/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 490478225214.2189 - val_loss: 547555997215.4995\n",
      "Epoch 14/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 487387673541.1057 - val_loss: 545847830214.0730\n",
      "Epoch 15/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 487832784113.2764 - val_loss: 540425434228.3879\n",
      "Epoch 16/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 482697079334.9462 - val_loss: 537104810103.5912\n",
      "Epoch 17/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 479347264540.4972 - val_loss: 536163484826.8279\n",
      "Epoch 18/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 475961557037.5955 - val_loss: 531088775891.9541\n",
      "Epoch 19/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 472892518726.7681 - val_loss: 527781857487.1491\n",
      "Epoch 20/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 469682147487.5844 - val_loss: 524259304679.7080\n",
      "Epoch 21/300\n",
      "8624/8624 [==============================] - 1s 80us/step - loss: 465915103099.0129 - val_loss: 528092081368.7591\n",
      "Epoch 22/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 461748274063.9109 - val_loss: 516865646942.2315\n",
      "Epoch 23/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 457350725662.3970 - val_loss: 515956397095.5078\n",
      "Epoch 24/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 453186512443.8442 - val_loss: 516508151564.5464\n",
      "Epoch 25/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 449150969327.8516 - val_loss: 501648657564.9635\n",
      "Epoch 26/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 441903966910.9313 - val_loss: 506674573171.0532\n",
      "Epoch 27/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 441696239707.1911 - val_loss: 491133386615.3243\n",
      "Epoch 28/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 434520361070.1892 - val_loss: 485060437479.9750\n",
      "Epoch 29/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 430113805268.3043 - val_loss: 493681116383.1658\n",
      "Epoch 30/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 426863235140.3934 - val_loss: 474917739612.8968\n",
      "Epoch 31/300\n",
      "8624/8624 [==============================] - 1s 77us/step - loss: 419762559656.1335 - val_loss: 470974841634.9698\n",
      "Epoch 32/300\n",
      "8624/8624 [==============================] - 1s 77us/step - loss: 414605313063.8961 - val_loss: 481515198009.1262\n",
      "Epoch 33/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 413915560768.1188 - val_loss: 460587139020.7466\n",
      "Epoch 34/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 409015473655.4508 - val_loss: 459464656556.4463\n",
      "Epoch 35/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 405619602555.4880 - val_loss: 456152550403.2033\n",
      "Epoch 36/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 403151403718.5306 - val_loss: 447803533493.5224\n",
      "Epoch 37/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 399645590111.9406 - val_loss: 444258424137.9437\n",
      "Epoch 38/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 394325388225.3062 - val_loss: 446969576908.2127\n",
      "Epoch 39/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 393355599188.0668 - val_loss: 436989133876.3212\n",
      "Epoch 40/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 389149590203.1317 - val_loss: 436524126400.2002\n",
      "Epoch 41/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 386998413756.5566 - val_loss: 432961424301.7810\n",
      "Epoch 42/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 387218656922.8349 - val_loss: 431526709813.9229\n",
      "Epoch 43/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 383857509963.0427 - val_loss: 432986989628.8634\n",
      "Epoch 44/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 380244453881.3506 - val_loss: 421197787917.6142\n",
      "Epoch 45/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 376667943755.5176 - val_loss: 434263827619.3702\n",
      "Epoch 46/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 377116703886.4861 - val_loss: 417768921094.4067\n",
      "Epoch 47/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 373185775422.2189 - val_loss: 412490830904.5923\n",
      "Epoch 48/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 368424600494.3080 - val_loss: 414394638583.7247\n",
      "Epoch 49/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 366559639077.0463 - val_loss: 404544571206.2065\n",
      "Epoch 50/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 362123312437.6697 - val_loss: 408930919023.5829\n",
      "Epoch 51/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 359399452117.2542 - val_loss: 397341626628.5381\n",
      "Epoch 52/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 356544929168.8609 - val_loss: 393659890977.3681\n",
      "Epoch 53/300\n",
      "8624/8624 [==============================] - 1s 82us/step - loss: 354454532392.3711 - val_loss: 394028802329.8936\n",
      "Epoch 54/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 351042646008.4008 - val_loss: 405389562159.2492\n",
      "Epoch 55/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 346628852870.8868 - val_loss: 387728110315.4453\n",
      "Epoch 56/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 342061334710.3822 - val_loss: 379399763709.5975\n",
      "Epoch 57/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 337789016455.3618 - val_loss: 390273540651.2451\n",
      "Epoch 58/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 337341018894.7236 - val_loss: 373744348530.5193\n",
      "Epoch 59/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 330778378209.6030 - val_loss: 368430026722.1022\n",
      "Epoch 60/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 328253356833.7217 - val_loss: 381081496758.5902\n",
      "Epoch 61/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 323949760914.7606 - val_loss: 363890194173.5975\n",
      "Epoch 62/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 319878668683.1614 - val_loss: 372317589730.3691\n",
      "Epoch 63/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 316765023549.2690 - val_loss: 377871095874.2023\n",
      "Epoch 64/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 309809370448.2672 - val_loss: 341700901425.6517\n",
      "Epoch 65/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 306000217751.0352 - val_loss: 356395467554.9698\n",
      "Epoch 66/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 300508008318.8126 - val_loss: 331954822356.4880\n",
      "Epoch 67/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 297289431993.7069 - val_loss: 344510467570.6528\n",
      "Epoch 68/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 288428434350.3080 - val_loss: 328755294733.3472\n",
      "Epoch 69/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 285206365892.6308 - val_loss: 315604079860.5214\n",
      "Epoch 70/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 280648322958.0111 - val_loss: 310868753594.8613\n",
      "Epoch 71/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 277307572482.3748 - val_loss: 306646942772.3212\n",
      "Epoch 72/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 269435267516.5566 - val_loss: 300468890054.8738\n",
      "Epoch 73/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 266150607383.7477 - val_loss: 340111564466.8530\n",
      "Epoch 74/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 261759862833.3952 - val_loss: 289970871405.9812\n",
      "Epoch 75/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 255926217938.8794 - val_loss: 279926339633.1179\n",
      "Epoch 76/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 250955550644.0074 - val_loss: 284093149846.0229\n",
      "Epoch 77/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 243076912958.2189 - val_loss: 269439744151.6246\n",
      "Epoch 78/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 241282623674.1818 - val_loss: 268217365082.2273\n",
      "Epoch 79/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 234527121468.7941 - val_loss: 265482139056.4505\n",
      "Epoch 80/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 233374423720.1336 - val_loss: 271163418873.8603\n",
      "Epoch 81/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 225733822011.8441 - val_loss: 252350322506.4776\n",
      "Epoch 82/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 221179095803.7254 - val_loss: 250283238238.7654\n",
      "Epoch 83/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 219532041510.4713 - val_loss: 242349429133.2138\n",
      "Epoch 84/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 216583384176.0891 - val_loss: 235028648862.8321\n",
      "Epoch 85/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 218834872065.4249 - val_loss: 238155304417.5683\n",
      "Epoch 86/300\n",
      "8624/8624 [==============================] - 1s 82us/step - loss: 204846311093.4323 - val_loss: 234790586130.9531\n",
      "Epoch 87/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 209011105419.6364 - val_loss: 226474626785.8352\n",
      "Epoch 88/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 198543173894.1744 - val_loss: 228999978521.0928\n",
      "Epoch 89/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 206094560466.8794 - val_loss: 262599585890.2357\n",
      "Epoch 90/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 200718482513.6920 - val_loss: 219099168631.3243\n",
      "Epoch 91/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 194772759082.7458 - val_loss: 213345486303.4327\n",
      "Epoch 92/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 196858332709.0464 - val_loss: 213435535073.8352\n",
      "Epoch 93/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 195736656324.1559 - val_loss: 210445934395.5287\n",
      "Epoch 94/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 190355288314.7755 - val_loss: 208196080491.5787\n",
      "Epoch 95/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 185313787714.0186 - val_loss: 210077067308.8467\n",
      "Epoch 96/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 192769114446.3674 - val_loss: 209142035179.4453\n",
      "Epoch 97/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 182797903252.6605 - val_loss: 225315372065.1011\n",
      "Epoch 98/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 187188923663.6735 - val_loss: 199804697445.1721\n",
      "Epoch 99/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 184452763967.1688 - val_loss: 201050638816.5005\n",
      "Epoch 100/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 180498658746.6568 - val_loss: 236136903352.1919\n",
      "Epoch 101/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 180936622201.5881 - val_loss: 229899319797.8561\n",
      "Epoch 102/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 178786493244.3191 - val_loss: 194334509741.5141\n",
      "Epoch 103/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 174576161702.7087 - val_loss: 224741502954.6444\n",
      "Epoch 104/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 183944253075.2356 - val_loss: 194664602938.9948\n",
      "Epoch 105/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 182754888519.7180 - val_loss: 196217613118.7320\n",
      "Epoch 106/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 179510550315.2208 - val_loss: 207765865455.9833\n",
      "Epoch 107/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 181864581230.1892 - val_loss: 190049500603.1283\n",
      "Epoch 108/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 177079694695.0649 - val_loss: 188871978303.2659\n",
      "Epoch 109/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 172700584661.7291 - val_loss: 192164064361.7101\n",
      "Epoch 110/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 169150656124.4378 - val_loss: 192428993436.6966\n",
      "Epoch 111/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 178124918282.4490 - val_loss: 215425347989.7560\n",
      "Epoch 112/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 170454849585.3952 - val_loss: 204037083541.7560\n",
      "Epoch 113/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 172185489894.3525 - val_loss: 183603496671.6997\n",
      "Epoch 114/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 173145428560.7421 - val_loss: 183837499708.0626\n",
      "Epoch 115/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 172286711371.0427 - val_loss: 213150566871.9583\n",
      "Epoch 116/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 169360210341.7588 - val_loss: 186172312340.0208\n",
      "Epoch 117/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 171816809169.9295 - val_loss: 197048594478.9823\n",
      "Epoch 118/300\n",
      "8624/8624 [==============================] - 1s 79us/step - loss: 166255504511.2876 - val_loss: 181618822113.0344\n",
      "Epoch 119/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 177637869997.3581 - val_loss: 182047108859.4619\n",
      "Epoch 120/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 164405086044.6159 - val_loss: 193165172923.9291\n",
      "Epoch 121/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 167153101160.9648 - val_loss: 189283768172.6465\n",
      "Epoch 122/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 164574777585.2765 - val_loss: 178475551490.9364\n",
      "Epoch 123/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 165050268024.1632 - val_loss: 305190238393.7935\n",
      "Epoch 124/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 170186316554.9239 - val_loss: 186209172504.5589\n",
      "Epoch 125/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 165394086096.9796 - val_loss: 229620294059.1116\n",
      "Epoch 126/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 161562116635.5473 - val_loss: 178628919688.9427\n",
      "Epoch 127/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 166075349360.5640 - val_loss: 185376234383.8832\n",
      "Epoch 128/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 159475226491.0130 - val_loss: 177717114433.6684\n",
      "Epoch 129/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 159829545104.3859 - val_loss: 179270814748.8300\n",
      "Epoch 130/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 169855410776.3414 - val_loss: 176187071871.3326\n",
      "Epoch 131/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 160270377096.7866 - val_loss: 176920668973.6476\n",
      "Epoch 132/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 165116524532.6011 - val_loss: 179658789987.3034\n",
      "Epoch 133/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 163281046077.7440 - val_loss: 182416737947.3618\n",
      "Epoch 134/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 161353640555.3395 - val_loss: 186688772230.5402\n",
      "Epoch 135/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 169908530166.5009 - val_loss: 195144135651.1700\n",
      "Epoch 136/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 160329525183.4063 - val_loss: 189032706522.0938\n",
      "Epoch 137/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 160416720405.8479 - val_loss: 188996318503.7748\n",
      "Epoch 138/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 160064990114.9091 - val_loss: 171359009509.0386\n",
      "Epoch 139/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 158431399510.4416 - val_loss: 173211993285.5391\n",
      "Epoch 140/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 153920271179.5176 - val_loss: 171313225770.7112\n",
      "Epoch 141/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 157618207764.8979 - val_loss: 192483867125.8561\n",
      "Epoch 142/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 170033906135.1540 - val_loss: 174832324748.9468\n",
      "Epoch 143/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 164025533122.7310 - val_loss: 178994958254.8488\n",
      "Epoch 144/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 159711919444.0668 - val_loss: 169406042099.1866\n",
      "Epoch 145/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 160393687549.1503 - val_loss: 173428314816.7341\n",
      "Epoch 146/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 157915697744.7421 - val_loss: 169485440210.3524\n",
      "Epoch 147/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 162819211636.3636 - val_loss: 170872436554.4776\n",
      "Epoch 148/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 154980317073.8108 - val_loss: 198544967152.5172\n",
      "Epoch 149/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 161803862023.5992 - val_loss: 181262932657.7852\n",
      "Epoch 150/300\n",
      "8624/8624 [==============================] - 1s 81us/step - loss: 162904055420.4378 - val_loss: 172649683497.1095\n",
      "Epoch 151/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 158641868412.4378 - val_loss: 173869644636.6298\n",
      "Epoch 152/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 158103115453.0315 - val_loss: 167132289184.1668\n",
      "Epoch 153/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 153169594789.7588 - val_loss: 176295654516.3879\n",
      "Epoch 154/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 167539194779.3098 - val_loss: 178109845901.2138\n",
      "Epoch 155/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 152751740407.4508 - val_loss: 177452218109.5975\n",
      "Epoch 156/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 155768588291.7996 - val_loss: 172775120921.6267\n",
      "Epoch 157/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 153097286034.7607 - val_loss: 174649200644.2711\n",
      "Epoch 158/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 160659543649.8405 - val_loss: 174584515049.0428\n",
      "Epoch 159/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 152113679768.4601 - val_loss: 167499108330.6444\n",
      "Epoch 160/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 162457529357.2987 - val_loss: 173642091480.4922\n",
      "Epoch 161/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 155977186788.4527 - val_loss: 170710886342.3399\n",
      "Epoch 162/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 157386006847.1688 - val_loss: 177498086561.2346\n",
      "Epoch 163/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 151330316656.5640 - val_loss: 186665596182.6903\n",
      "Epoch 164/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 155905742076.6753 - val_loss: 168621492422.6069\n",
      "Epoch 165/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 154391114487.9258 - val_loss: 196687840077.6809\n",
      "Epoch 166/300\n",
      "8624/8624 [==============================] - 1s 77us/step - loss: 154193057210.6568 - val_loss: 191147088954.7278\n",
      "Epoch 167/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 156796308119.0352 - val_loss: 165991071470.6486\n",
      "Epoch 168/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 159567331593.9740 - val_loss: 169695127767.6913\n",
      "Epoch 169/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 151936289337.9443 - val_loss: 165028806937.8936\n",
      "Epoch 170/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 155742694291.7106 - val_loss: 167266673297.7518\n",
      "Epoch 171/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 148279847880.9054 - val_loss: 189631889561.7602\n",
      "Epoch 172/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 157386617242.3599 - val_loss: 169034274480.7174\n",
      "Epoch 173/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 152236390027.6364 - val_loss: 193953592094.6986\n",
      "Epoch 174/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 151580058337.1280 - val_loss: 168724752292.1710\n",
      "Epoch 175/300\n",
      "8624/8624 [==============================] - 1s 77us/step - loss: 150767065152.5937 - val_loss: 172955832392.6090\n",
      "Epoch 176/300\n",
      "8624/8624 [==============================] - 1s 83us/step - loss: 152326420977.7514 - val_loss: 163530649629.8978\n",
      "Epoch 177/300\n",
      "8624/8624 [==============================] - 1s 84us/step - loss: 154853956028.5566 - val_loss: 213773854636.7133\n",
      "Epoch 178/300\n",
      "8624/8624 [==============================] - 1s 83us/step - loss: 150689345566.3970 - val_loss: 175001710442.5110\n",
      "Epoch 179/300\n",
      "8624/8624 [==============================] - 1s 82us/step - loss: 152870405361.2765 - val_loss: 164344273807.8832\n",
      "Epoch 180/300\n",
      "8624/8624 [==============================] - 1s 83us/step - loss: 154381605422.5455 - val_loss: 170050443936.7007\n",
      "Epoch 181/300\n",
      "8624/8624 [==============================] - 1s 84us/step - loss: 151088979857.8108 - val_loss: 168499562668.9802\n",
      "Epoch 182/300\n",
      "8624/8624 [==============================] - 1s 85us/step - loss: 152032292788.0074 - val_loss: 193431912635.9291\n",
      "Epoch 183/300\n",
      "8624/8624 [==============================] - 1s 86us/step - loss: 154470598424.2226 - val_loss: 182755044192.9009\n",
      "Epoch 184/300\n",
      "8624/8624 [==============================] - 1s 85us/step - loss: 162473205615.6141 - val_loss: 166227098341.0386\n",
      "Epoch 185/300\n",
      "8624/8624 [==============================] - 1s 84us/step - loss: 150904975420.7941 - val_loss: 166611053818.9280\n",
      "Epoch 186/300\n",
      "8624/8624 [==============================] - 1s 83us/step - loss: 154351473548.1113 - val_loss: 162329722341.8394\n",
      "Epoch 187/300\n",
      "8624/8624 [==============================] - 1s 86us/step - loss: 151126911006.3970 - val_loss: 196541123834.9280\n",
      "Epoch 188/300\n",
      "8624/8624 [==============================] - 1s 90us/step - loss: 148279535308.2301 - val_loss: 160773838598.1397\n",
      "Epoch 189/300\n",
      "8624/8624 [==============================] - 1s 84us/step - loss: 151486271480.4008 - val_loss: 166221548162.8029\n",
      "Epoch 190/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 149324282558.9314 - val_loss: 184363099162.6945\n",
      "Epoch 191/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 152716555942.2338 - val_loss: 180203424600.3587\n",
      "Epoch 192/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 150966944222.7532 - val_loss: 178615110317.5141\n",
      "Epoch 193/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 154083841953.0093 - val_loss: 172074200844.5464\n",
      "Epoch 194/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 151692825195.3395 - val_loss: 183387949365.6559\n",
      "Epoch 195/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 148608923496.0148 - val_loss: 162362396240.6173\n",
      "Epoch 196/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 149331137081.9443 - val_loss: 162319108456.9093\n",
      "Epoch 197/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 150439170139.1911 - val_loss: 162004798287.8165\n",
      "Epoch 198/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 151197892513.0093 - val_loss: 181304589430.5235\n",
      "Epoch 199/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 152998099396.1559 - val_loss: 185152139268.2711\n",
      "Epoch 200/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 146737906294.7384 - val_loss: 171887656438.9239\n",
      "Epoch 201/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 147858974298.2412 - val_loss: 165592316798.7987\n",
      "Epoch 202/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 146787283339.1614 - val_loss: 160278690618.4609\n",
      "Epoch 203/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 148650539634.9388 - val_loss: 184265415807.0657\n",
      "Epoch 204/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 146821946822.0557 - val_loss: 160351580113.0177\n",
      "Epoch 205/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 149954847666.1076 - val_loss: 162481825871.0157\n",
      "Epoch 206/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 146257097760.2968 - val_loss: 166867807944.2086\n",
      "Epoch 207/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 148639245762.2560 - val_loss: 161497255914.6444\n",
      "Epoch 208/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 145739986537.4397 - val_loss: 161085442878.7320\n",
      "Epoch 209/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 145176008251.8441 - val_loss: 170819086252.7133\n",
      "Epoch 210/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 151566668026.7755 - val_loss: 159803499973.8061\n",
      "Epoch 211/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 155080449979.6067 - val_loss: 170045220182.7570\n",
      "Epoch 212/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 148978714042.6568 - val_loss: 168255922056.4088\n",
      "Epoch 213/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 146529296368.8015 - val_loss: 178168294668.0125\n",
      "Epoch 214/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 150769150784.1187 - val_loss: 179992697111.7581\n",
      "Epoch 215/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 149822335808.1187 - val_loss: 158549132972.4463\n",
      "Epoch 216/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 150703629830.6494 - val_loss: 158523476978.1189\n",
      "Epoch 217/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 153330664928.6531 - val_loss: 167398273908.1210\n",
      "Epoch 218/300\n",
      "8624/8624 [==============================] - 1s 77us/step - loss: 152311295711.2282 - val_loss: 159914120376.7258\n",
      "Epoch 219/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 149911367710.3970 - val_loss: 159391608110.1814\n",
      "Epoch 220/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 145027170304.0000 - val_loss: 158374481514.2440\n",
      "Epoch 221/300\n",
      "8624/8624 [==============================] - 1s 80us/step - loss: 150858478415.3172 - val_loss: 162191060936.4755\n",
      "Epoch 222/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 142757777445.9963 - val_loss: 190742017960.4421\n",
      "Epoch 223/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 157111395557.8775 - val_loss: 158904742831.9166\n",
      "Epoch 224/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 150601364303.3172 - val_loss: 162877545436.7633\n",
      "Epoch 225/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 144643184583.0056 - val_loss: 170503635567.5829\n",
      "Epoch 226/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 142988125307.4879 - val_loss: 185571561318.2398\n",
      "Epoch 227/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 144581405040.5640 - val_loss: 165901616291.3702\n",
      "Epoch 228/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 144547843404.4675 - val_loss: 161969005876.5881\n",
      "Epoch 229/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 163601550873.6475 - val_loss: 167680139152.9510\n",
      "Epoch 230/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 150589004433.3358 - val_loss: 165688317659.4286\n",
      "Epoch 231/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 145104270239.1095 - val_loss: 160363141647.4828\n",
      "Epoch 232/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 145400057058.0779 - val_loss: 160616998948.3045\n",
      "Epoch 233/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 147678981954.0186 - val_loss: 155949305826.1022\n",
      "Epoch 234/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 144643996100.1559 - val_loss: 159697006493.7643\n",
      "Epoch 235/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 147182479652.5714 - val_loss: 173114089711.1825\n",
      "Epoch 236/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 150323507002.4193 - val_loss: 156022671883.2117\n",
      "Epoch 237/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 149317705378.4341 - val_loss: 170834978758.3399\n",
      "Epoch 238/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 151177310382.7829 - val_loss: 162000494933.6893\n",
      "Epoch 239/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 141145877076.5417 - val_loss: 174756607041.1345\n",
      "Epoch 240/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 146630748171.3989 - val_loss: 162599769879.2242\n",
      "Epoch 241/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 154495728729.2913 - val_loss: 157926218998.6569\n",
      "Epoch 242/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 144043198946.5529 - val_loss: 164506220840.8425\n",
      "Epoch 243/300\n",
      "8624/8624 [==============================] - 1s 73us/step - loss: 143739698485.6698 - val_loss: 192986696228.8384\n",
      "Epoch 244/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 146167354787.8590 - val_loss: 156168069085.8311\n",
      "Epoch 245/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 146260999128.1039 - val_loss: 174907148392.6423\n",
      "Epoch 246/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 144474289883.4286 - val_loss: 155383560753.6517\n",
      "Epoch 247/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 148854572603.8441 - val_loss: 161798310616.2252\n",
      "Epoch 248/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 143802855872.3562 - val_loss: 189102433157.2054\n",
      "Epoch 249/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 148022255023.2579 - val_loss: 160689624180.3879\n",
      "Epoch 250/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 146634381710.9611 - val_loss: 213945416984.8259\n",
      "Epoch 251/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 142223907380.2449 - val_loss: 157578268206.4484\n",
      "Epoch 252/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 140480735099.0130 - val_loss: 155646749758.9990\n",
      "Epoch 253/300\n",
      "8624/8624 [==============================] - 1s 79us/step - loss: 144482831380.8979 - val_loss: 161393092430.7487\n",
      "Epoch 254/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 144011290278.2338 - val_loss: 153926756379.7622\n",
      "Epoch 255/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 143853850479.6141 - val_loss: 162132980741.3389\n",
      "Epoch 256/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 145535065982.8126 - val_loss: 156630379121.7184\n",
      "Epoch 257/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 147822944069.8182 - val_loss: 201585478539.6121\n",
      "Epoch 258/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 141920129806.7235 - val_loss: 163103660516.7716\n",
      "Epoch 259/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 140685106637.6549 - val_loss: 171272957896.4755\n",
      "Epoch 260/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 144960222133.9072 - val_loss: 153591345796.9385\n",
      "Epoch 261/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 148332820673.7811 - val_loss: 158277560407.5579\n",
      "Epoch 262/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 149253126125.0019 - val_loss: 156262303592.3754\n",
      "Epoch 263/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 142392359554.1373 - val_loss: 163053975823.2159\n",
      "Epoch 264/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 141934574894.0705 - val_loss: 153923860631.6246\n",
      "Epoch 265/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 139905301167.7328 - val_loss: 155251897614.1481\n",
      "Epoch 266/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 140824915990.7978 - val_loss: 161538414880.3003\n",
      "Epoch 267/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 141811919459.7402 - val_loss: 179423288865.6350\n",
      "Epoch 268/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 149744187357.8033 - val_loss: 211383834886.6736\n",
      "Epoch 269/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 146287113195.1021 - val_loss: 159305418514.9531\n",
      "Epoch 270/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 154017370203.1911 - val_loss: 153086095475.3201\n",
      "Epoch 271/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 139481652662.8571 - val_loss: 155968866707.6205\n",
      "Epoch 272/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 141992384025.6475 - val_loss: 154062296265.8102\n",
      "Epoch 273/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 138950628866.8497 - val_loss: 164298817723.9291\n",
      "Epoch 274/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 142247067374.4267 - val_loss: 152522655353.1929\n",
      "Epoch 275/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 142282240676.3340 - val_loss: 149633724052.9552\n",
      "Epoch 276/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 147623450756.9870 - val_loss: 150747182563.7039\n",
      "Epoch 277/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 142375267212.1113 - val_loss: 181848015753.4765\n",
      "Epoch 278/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 141685381716.5417 - val_loss: 151345917887.9333\n",
      "Epoch 279/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 140230338468.8089 - val_loss: 170605958081.0010\n",
      "Epoch 280/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 147060225405.8627 - val_loss: 195590476241.5516\n",
      "Epoch 281/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 141838346433.7811 - val_loss: 149301157845.2888\n",
      "Epoch 282/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 141195462317.8330 - val_loss: 150951482774.8238\n",
      "Epoch 283/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 142659411892.0074 - val_loss: 168432312618.9781\n",
      "Epoch 284/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 139614350113.7217 - val_loss: 151564377052.7633\n",
      "Epoch 285/300\n",
      "8624/8624 [==============================] - 1s 79us/step - loss: 136730964313.7662 - val_loss: 146678894929.4182\n",
      "Epoch 286/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 137378994535.0649 - val_loss: 177137077165.7810\n",
      "Epoch 287/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 140839599763.2356 - val_loss: 149505765465.6934\n",
      "Epoch 288/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 142941709511.4805 - val_loss: 152261708837.3723\n",
      "Epoch 289/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 141603211719.9555 - val_loss: 166530791169.8686\n",
      "Epoch 290/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 138079182262.8571 - val_loss: 145980553213.8644\n",
      "Epoch 291/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 145040487273.9147 - val_loss: 157651305900.1794\n",
      "Epoch 292/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 138426687662.7829 - val_loss: 145390991810.6027\n",
      "Epoch 293/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 137078148451.2653 - val_loss: 150782850779.4286\n",
      "Epoch 294/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 134873386711.6289 - val_loss: 144382361634.1689\n",
      "Epoch 295/300\n",
      "8624/8624 [==============================] - 1s 76us/step - loss: 136998157188.5121 - val_loss: 152492245273.8936\n",
      "Epoch 296/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 139425979625.6772 - val_loss: 145018328402.4859\n",
      "Epoch 297/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 135621357545.2022 - val_loss: 147530220775.7080\n",
      "Epoch 298/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 135177690611.6512 - val_loss: 145001973420.4463\n",
      "Epoch 299/300\n",
      "8624/8624 [==============================] - 1s 74us/step - loss: 138603570679.4508 - val_loss: 149580288209.2847\n",
      "Epoch 300/300\n",
      "8624/8624 [==============================] - 1s 75us/step - loss: 138680446330.0631 - val_loss: 149747037343.0991\n",
      "R2 score: 0.8227201063212397\n",
      "RMSE Score: 430936.38953920116\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_Neural = Sequential()\n",
    "\n",
    "# get # of columns in training data\n",
    "cols = X_train.shape[1]\n",
    "\n",
    "#adding layers\n",
    "model_Neural.add(Dense(200, activation='relu', input_shape=(cols,)))\n",
    "model_Neural.add(Dense(100, activation='relu'))\n",
    "model_Neural.add(Dense(30, activation='relu'))\n",
    "model_Neural.add(Dense(1))\n",
    "\n",
    "model_Neural.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=70)\n",
    "\n",
    "model_Neural.fit(X_train, y_train, validation_split=0.1, epochs=300,callbacks=[early_stopping_monitor])\n",
    "\n",
    "predictions = model_Neural.predict(X_test)\n",
    "print(\"R2 score:\",r2_score(y_test, predictions))\n",
    "rms = sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"RMSE Score:\",rms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oWGyi9jR-K09"
   },
   "outputs": [],
   "source": [
    "filename = 'nn_f4.pkl'\n",
    "model_Neural.save('nn_f4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Kt1ytAS-K5c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k303wnlV-Ktb"
   },
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame()\n",
    "\n",
    "for i in df.index:\n",
    "  \n",
    "  x = df.loc[i,df.columns[1:]]\n",
    "  x = np.array(x).reshape(1,-1)\n",
    "  \n",
    "  ndf.loc[i,'price'] = df.loc[i,'price']\n",
    "  ndf.loc[i,'f1'] = f1.predict(x)[0]\n",
    "  ndf.loc[i,'f2'] = f2.predict(x)[0]\n",
    "  ndf.loc[i,'f3'] = f3.predict(x)[0]\n",
    "  ndf.loc[i,'f4'] = f4.predict(x)[0][0]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xy7JUUiolGfW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1wGVl4MjlGrz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smMQpjesaS86"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMBt8KuxaTAk"
   },
   "outputs": [],
   "source": [
    "X = ndf.drop(['price'],axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ndf['price'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1554352325278,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "zbPNrNguaTEp",
    "outputId": "e5c76ee6-6915-4fd3-fb24-092b2a61c095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression Scores- rmse: 312813.0787349889  r2: 0.9020859017485927\n"
     ]
    }
   ],
   "source": [
    "lm_model = LinearRegression(normalize=True)\n",
    "lm_model.fit(X_train,y_train)\n",
    "y_pred = lm_model.predict(X_test)\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test, y_pred)) \n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Linear regression Scores- rmse:\",rmse,\" r2:\",r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AEasFfsZ-KoF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 597,
     "status": "ok",
     "timestamp": 1554353486042,
     "user": {
      "displayName": "Pavan Kosaraju",
      "photoUrl": "https://lh6.googleusercontent.com/-ILNurn_VJZ0/AAAAAAAAAAI/AAAAAAAAATo/quTWto_Z3WI/s64/photo.jpg",
      "userId": "07110276984248519689"
     },
     "user_tz": 420
    },
    "id": "D6bmVKb_nVno",
    "outputId": "dbce4d6a-e32a-4726-b8be-3d3669ccbc24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted price: 552918.29\n",
      "actual price:  518800\n"
     ]
    }
   ],
   "source": [
    "i=66\n",
    "\n",
    "x = ndf.loc[i,ndf.columns[1:]]\n",
    "x = np.array(x).reshape(1,-1)\n",
    "\n",
    "print('predicted price: %.2f'%lm_model.predict(x)[0])\n",
    "print('actual price: ',df.loc[i,'price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXMv-S2LpJ4R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zO5yN-zipJ61"
   },
   "outputs": [],
   "source": [
    "filename = 'finalmodel.pkl'\n",
    "pickle.dump(lm_model, open(filename, 'wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = ['V3S', 'V7C', 'V3W', 'V4N', 'V6Y', 'V6X', 'V4A', 'V2X', 'V7E', 'V3B','V3T', 'V4B', 'V3R', 'V3A', 'V6B',\n",
    "         'V5R', 'V3M', 'V5H','V3E', 'V2Y', 'V3Z', 'V7S', 'V6P', 'V6Z', 'V7A', 'V3K', 'V3J', 'V3N', 'V7L', 'V3H',\n",
    "         'V3C', 'V4C', 'V3X', 'V5J', 'V1M', 'V3V', 'V5N', 'V6E', 'V5E', 'V7V', 'V4P', 'V5C', 'V4K', 'V4R', 'V5X',\n",
    "         'V5Z', 'V2W', 'V6S','V2Z', 'V6M', 'V3L', 'V7M', 'V6G', 'V5M', 'V7W', 'V5A', 'V4M', 'V6N', 'V5Y', 'V5S',\n",
    "         'V7R', 'V7J', 'V5P', 'V6R', 'V7T', 'V5B', 'V7P', 'V3Y', 'V4W', 'V6J', 'V4L', 'V7G', 'V6K', 'V6A', 'V5G',\n",
    "         'V5T', 'V7N','V4E', 'V6H', 'V6V', 'V5K', 'V6T', 'V6C', 'V5L', 'V6L', 'V5V', 'V5W', 'V7H', 'V7K', 'V6W',\n",
    "         'V0N', 'V0V', 'V7B', 'V3G', 'V0X', 'V2S', 'V2T', 'V4S', 'V4X', 'V1V', 'V2E', 'V0T', 'V9L', 'V8K', 'V0H',\n",
    "         'V0Y','V0M', 'V8V', 'V2J', 'V2A', 'V1L', 'V2H', 'V0B', 'V0L', 'V4G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimep = {'V6E':9.652823,'V6H':8.577192,'V5T':8.09922,'V5M':6.480475,'V5L':6.361571,'V6J':6.026049,'V5N':5.794912,\n",
    "          'V6A':5.666982,'V5K':4.266424,'V6M':3.76216,'V5W':3.643648,'V6L':3.232389,'V5V':3.110738,'V6P':2.989087,\n",
    "          'V5P':2.39692,'V6S':2.266636,'V6R':1.380544,'V5Z':1.316187,'V6G':0.784061,'V6N':0.096536}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7_0yurVrpJ8X"
   },
   "outputs": [],
   "source": [
    "srs = {'V3R':5.214286,'V3S':5.940625,'V3T':4.837500,'V3V':3.827273,'V3W':5.425000,'V3X':5.785714,'V4A':6.916667,'V4N':6.286364,\n",
    "       'V4P':7.000000,'V5K':7.040000,'V5L':5.450000,'V5M':7.140000,'V5N':6.883333,'V5P':6.216667,'V5R':6.800000,'V5S':6.166667,\n",
    "       'V5T':8.000000,'V5V':5.933333,'V5W':5.425000,'V5X':6.650000,'V5Z':7.600000,'V6B':6.800000,'V6G':5.500000,'V6H':7.600000,\n",
    "       'V6J':8.280000,'V6K':7.875000,'V6L':6.475000,'V6M':7.280000,'V6N':7.466667,'V6P':7.660000,'V6R':7.657143,'V6S':9.400000,\n",
    "       'V6T':6.550000,'V6Z':6.600000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l1hy2Sf1nVl2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for Front-end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator DecisionTreeRegressor from version 0.20.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator GradientBoostingRegressor from version 0.20.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:311: UserWarning: Trying to unpickle estimator LinearRegression from version 0.20.3 when using version 0.19.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load models\n",
    "\n",
    "f1n = 'svr_f1.pkl' # # Try svr_f1_new.pkl if predict_price function fails\n",
    "f2n = 'gbr_f2.pkl'\n",
    "f3n = 'rfr_f3.pkl' # Try rfr_f3_new.pkl if predict_price function fails\n",
    "f4n = 'nn_f4.h5'\n",
    "finall = 'finalmodel.pkl'\n",
    "\n",
    "f1 = pickle.load(open(f1n, 'rb'))\n",
    "f2 = pickle.load(open(f2n, 'rb'))\n",
    "f3 = pickle.load(open(f3n, 'rb'))\n",
    "f4 = load_model(f4n)\n",
    "stackedmodel = pickle.load(open(finall, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(htype,parea):\n",
    "    \n",
    "    encoded = []\n",
    "    \n",
    "    # Add Crime percentage\n",
    "    \n",
    "    crimep = {'V6E':9.652823,'V6H':8.577192,'V5T':8.09922,'V5M':6.480475,'V5L':6.361571,'V6J':6.026049,'V5N':5.794912,\n",
    "              'V6A':5.666982,'V5K':4.266424,'V6M':3.76216,'V5W':3.643648,'V6L':3.232389,'V5V':3.110738,'V6P':2.989087,\n",
    "              'V5P':2.39692,'V6S':2.266636,'V6R':1.380544,'V5Z':1.316187,'V6G':0.784061,'V6N':0.096536}\n",
    "    \n",
    "    if parea in crimep.keys():\n",
    "        encoded.append(crimep[parea])\n",
    "    else:\n",
    "        encoded.append(0.0)\n",
    "    \n",
    "    # Encoding House type\n",
    "    \n",
    "    if htype in ['Apt/Condo','Mfd/Mobile Home','Townhouse']:\n",
    "        encoded.append(0)\n",
    "        encoded.append(1)\n",
    "    else:\n",
    "        encoded.append(1)\n",
    "        encoded.append(0)\n",
    "        \n",
    "    \n",
    "    # Encoding School type\n",
    "    \n",
    "    srs = {'V4A':[0,1,0],'V4N':[0,1,0],'V4P':[0,1,0],'V5K':[0,1,0],'V5M':[0,1,0],'V5N':[0,1,0],'V5P':[0,1,0],'V5R':[0,1,0],\n",
    "           'V5S':[0,1,0],'V5T':[0,1,0],'V5X':[0,1,0],'V5Z':[0,1,0],'V6B':[0,1,0],'V6H':[0,1,0],'V6J':[1,0,0],'V6K':[0,1,0],\n",
    "           'V6L':[0,1,0],'V6M':[0,1,0],'V6N':[0,1,0],'V6P':[0,1,0],'V6R':[0,1,0],'V6S':[1,0,0],'V6T':[0,1,0],'V6Z':[0,1,0]}\n",
    "    \n",
    "    if parea not in srs.keys():\n",
    "        encoded = encoded + [0,0,1]\n",
    "    else:\n",
    "        encoded = encoded + srs[parea]\n",
    "        \n",
    "    # Encoding Area\n",
    "            \n",
    "    areas = ['V3S', 'V7C', 'V3W', 'V4N', 'V6Y', 'V6X', 'V4A', 'V2X', 'V7E', 'V3B','V3T', 'V4B', 'V3R', 'V3A', 'V6B',\n",
    "         'V5R', 'V3M', 'V5H','V3E', 'V2Y', 'V3Z', 'V7S', 'V6P', 'V6Z', 'V7A', 'V3K', 'V3J', 'V3N', 'V7L', 'V3H',\n",
    "         'V3C', 'V4C', 'V3X', 'V5J', 'V1M', 'V3V', 'V5N', 'V6E', 'V5E', 'V7V', 'V4P', 'V5C', 'V4K', 'V4R', 'V5X',\n",
    "         'V5Z', 'V2W', 'V6S','V2Z', 'V6M', 'V3L', 'V7M', 'V6G', 'V5M', 'V7W', 'V5A', 'V4M', 'V6N', 'V5Y', 'V5S',\n",
    "         'V7R', 'V7J', 'V5P', 'V6R', 'V7T', 'V5B', 'V7P', 'V3Y', 'V4W', 'V6J', 'V4L', 'V7G', 'V6K', 'V6A', 'V5G',\n",
    "         'V5T', 'V7N','V4E', 'V6H', 'V6V', 'V5K', 'V6T', 'V6C', 'V5L', 'V6L', 'V5V', 'V5W', 'V7H', 'V7K', 'V6W',\n",
    "         'V0N', 'V0V', 'V7B', 'V3G', 'V0X', 'V2S', 'V2T', 'V4S', 'V4X', 'V1V', 'V2E', 'V0T', 'V9L', 'V8K', 'V0H',\n",
    "         'V0Y','V0M', 'V8V', 'V2J', 'V2A', 'V1L', 'V2H', 'V0B', 'V0L', 'V4G']\n",
    "    \n",
    "    for i in areas:\n",
    "        if i == parea:\n",
    "            encoded.append(1)\n",
    "        else:\n",
    "            encoded.append(0)\n",
    "            \n",
    "    return encoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_price(bed,bath,area_sqft,age,fireplaces,housetype,area):\n",
    "    \n",
    "    inputs = [bed,bath,area_sqft,age,fireplaces]\n",
    "    inputs = inputs + encode_data(housetype,area)\n",
    "    sample = []\n",
    "    sample.append(tuple(inputs))\n",
    "    \n",
    "    inp_sample = pd.DataFrame(sample)\n",
    "    inp_sample = np.array(inp_sample).reshape(1,-1)\n",
    "    \n",
    "    p1 = f1.predict(inp_sample)[0]\n",
    "    p2 = f2.predict(inp_sample)[0]\n",
    "    p3 = f3.predict(inp_sample)[0]\n",
    "    p4 = f4.predict(inp_sample)[0][0]\n",
    "    \n",
    "    newInp = pd.DataFrame([(p1,p2,p3,p4)])\n",
    "    newInp = np.array(newInp).reshape(1,-1)\n",
    "    predicted_price = stackedmodel.predict(newInp)[0]\n",
    "    \n",
    "    return predicted_price\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs from front-end\n",
    "\n",
    "bedrooms = 3 # Any Integer\n",
    "bathrooms = 2 # Any Integer\n",
    "Area_sqft = 1800 # Any Integer\n",
    "builtin_last = 12 # Property built in last x years. Any Integer\n",
    "fireplaces_in_house = 1 # Any Integer\n",
    "house_type = 'Townhouse' # Can only be 'Apt/Condo','Mfd/Mobile Home','Townhouse','Duplex','House','Land/Lot','Multifamily'\n",
    "postal_area = 'V5J' # Postal code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted price: 1004599.99\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "p = predict_price(bedrooms,bathrooms,Area_sqft,builtin_last,fireplaces_in_house,house_type,postal_area)\n",
    "print('Predicted price: %.2f'%p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "REW_model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
