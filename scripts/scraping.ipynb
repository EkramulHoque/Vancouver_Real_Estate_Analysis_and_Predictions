{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = 1 # number of pages to scrape\n",
    "distance = 3 # search radius in km within an area\n",
    "\n",
    "#link = \"https://vancouver.craigslist.org/d/apts-housing-for-rent/search/apa\"\n",
    "path = \"pages/page_\"\n",
    "items=120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USER AGENTS\n",
    "\n",
    "# Checkout https://developers.whatismybrowser.com/useragents/explore/\n",
    "\n",
    "useragents = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.90 Safari/537.36',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.3; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.111 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36',\n",
    "    'Mozilla/5.0 (Linux; U; Android 4.1.2; de-de; GT-I8190 Build/JZO54K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30',\n",
    "    'Mozilla/5.0 (Linux; U; Android 4.1.2; de-de; ME371MG Build/JZO54K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30',\n",
    "    'Mozilla/5.0 (Linux; U; Android 4.2.2; pl-pl; GT-P5110 Build/JDQ39) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30',\n",
    "    'Mozilla/5.0 (Linux; U; Android 4.1.1; de-de; SGPT12 Build/TJDSU0177) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Safari/534.30',\n",
    "    'Opera/9.80 (J2ME/MIDP; Opera Mini/7.1.32052/29.3417; U; en) Presto/2.8.119 Version/11.10',\n",
    "    'Opera/9.80 (J2ME/MIDP; Opera Mini/7.1.32052/29.3594; U; en) Presto/2.8.119 Version/11.10',\n",
    "    'Opera/9.80 (J2ME/MIDP; Opera Mini/4.2.23449/28.2144; U; en) Presto/2.8.119 Version/11.10',\n",
    "    'Opera/9.80 (J2ME/MIDP; Opera Mini/4.2.20464/28.3950; U; en) Presto/2.8.119 Version/11.10',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:40.0) Gecko/20100101 Firefox/40.1',\n",
    "    'Mozilla/5.0 (Windows NT 5.1; rv:36.0) Gecko/20100101 Firefox/36.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:52.0) Gecko/20100101 Firefox/52.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:43.0) Gecko/20100101 Firefox/43.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; rv:54.0) Gecko/20100101 Firefox/54.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:57.0) Gecko/20100101 Firefox/57.0\t',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:63.0) Gecko/20100101 Firefox/63.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:61.0) Gecko/20100101 Firefox/61.0',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:59.0) Gecko/20100101 Firefox/59.0',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0',\n",
    "    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)',\n",
    "    'Mozilla/5.0 (Windows NT 6.1; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; WOW64; Trident/7.0; rv:11.0) like Gecko',\n",
    "    'Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 11_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 11_4_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/601.7.7 (KHTML, like Gecko) Version/9.1.2 Safari/601.7.7',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_5) AppleWebKit/603.3.8 (KHTML, like Gecko) Version/10.1.2 Safari/603.3.8',\n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/12.0 Mobile/15E148 Safari/604.1',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_4) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/11.1 Safari/605.1.15'    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free Proxies\n",
    "\n",
    "#https://free-proxy-list.net/\n",
    "\n",
    "def get_proxies():\n",
    "    proxies =[]\n",
    "    url = 'https://free-proxy-list.net/'\n",
    "    r = requests.get(url)\n",
    "    s = BeautifulSoup(r.content,'html.parser')\n",
    "    dt = s.find_all(\"tr\", limit=21)\n",
    "    for i in range(20):\n",
    "        if dt[i+1].find_all(\"td\")[4].text == \"elite proxy\":\n",
    "            px = dt[i+1].find_all(\"td\")[0].text + \":\" + dt[i+1].find_all(\"td\")[1].text\n",
    "            proxies.append(px)\n",
    "    return proxies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# RUN IT IF YOU WANT TO SAVE WEBPAGES\n",
    "\n",
    "# for i in range(pages):\n",
    "#     fname = path+str(i+1)+\".txt\"\n",
    "#     r = requests.get(link)\n",
    "#     open(fname, 'wb').write(r.content)\n",
    "#     #s = BeautifulSoup(r.content,'html.parser')\n",
    "#     #dt = s.find_all(\"li\",\"result-row\")\n",
    "#     link = \"https://vancouver.craigslist.org/search/apa?s=\"+str(items*(i+1))\n",
    "    \n",
    "    \n",
    "# def get_lat_lon(url,proxies,useragents):\n",
    "    \n",
    "#     user_agent = random.choice(useragents)\n",
    "#     headers = {'User-Agent': user_agent}\n",
    "#     proxy = random.choice(proxies)\n",
    "#     mapdt = requests.get(url,headers=headers,proxies={\"http\": proxy, \"https\": proxy})\n",
    "#     ss = BeautifulSoup(mapdt.content,'html.parser')\n",
    "#     dd = ss.find(\"div\",\"viewposting\")\n",
    "#     lat = dd.get(\"data-latitude\")\n",
    "#     long = dd.get(\"data-longitude\")\n",
    "#     return lat,long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_postal_codes():\n",
    "    \n",
    "    area = []\n",
    "    aname = []\n",
    "    l = \"https://en.wikipedia.org/wiki/List_of_postal_codes_of_Canada:_V\"\n",
    "    r = requests.get(l)\n",
    "    s = BeautifulSoup(r.content,'html.parser')\n",
    "    dt = s.find_all(\"td\",limit = 180)\n",
    "    for a in dt:\n",
    "        area.append(a.find(\"b\").text)\n",
    "        names = a.find_all(\"a\")\n",
    "        n = \"\"\n",
    "        for na in names:\n",
    "            n = n + na.text+\", \"\n",
    "        n = n.strip(\" ,\")\n",
    "        aname.append(n)\n",
    "    return area,aname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping rental properties in Burnaby, Government Road, Lake City, SFU, Burnaby Mountain region(s) -> Page 1\n",
      "-> Scrape successful.\n",
      "Scraping rental properties in Coquitlam region(s) -> Page 1\n",
      "-> Scrape successful.\n",
      "Scraping rental properties in Richmond region(s) -> Page 1\n",
      "-> Scrape successful.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('rental_data.csv', 'w',newline='') as csvfile:\n",
    "    \n",
    "    filewriter = csv.writer(csvfile, delimiter=',')\n",
    "    filewriter.writerow(['Post_ID', 'Time', 'House_type', 'Area', 'Area_Name', 'Address','Rent','url'])\n",
    "    #filewriter.writerow(['Post_ID', 'Time','lat','long', 'House_type', 'Address','Rent','url'])\n",
    "    \n",
    "\n",
    "    proxies = get_proxies()\n",
    "    areas,area_names = get_postal_codes()\n",
    "    areas = ['T5T', 'V5A', 'V3E','V7A']\n",
    "    area_names = ['',\n",
    "                  'Burnaby, Government Road, Lake City, SFU, Burnaby Mountain',\n",
    "                  'Coquitlam',\n",
    "                  'Richmond']\n",
    "    \n",
    "    st = np.random.randint(2, 10, size=len(areas))\n",
    "    updateproxy = np.random.randint(0, 2, size=len(areas))\n",
    "\n",
    "    for x in range(len(areas)):\n",
    "        \n",
    "        if area_names[x] == \"\":\n",
    "            continue\n",
    "        \n",
    "        link = \"https://vancouver.craigslist.org/search/apa?search_distance=\"+str(distance)+\"&postal=\"+areas[x]+\"&availabilityMode=0&sale_date=all+dates\"\n",
    "         \n",
    "        if updateproxy[x]:\n",
    "            proxies = get_proxies() \n",
    "        \n",
    "        user_agent = random.choice(useragents)\n",
    "        headers = {'User-Agent': user_agent}\n",
    "        proxy = random.choice(proxies)\n",
    "        \n",
    "        #print(str(x) +\"<-Loop Sleep->\"+str(st[x]))#############\n",
    "        time.sleep(st[x])\n",
    "        i = 0\n",
    "        total = 120\n",
    "        \n",
    "        while True:\n",
    "    \n",
    "    \n",
    "            i = i+1\n",
    "            print(\"Scraping rental properties in \"+area_names[x]+\" region(s) -> Page \"+str(i))\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    r = requests.get(link,headers=headers,proxies={\"http\": proxy, \"https\": proxy})\n",
    "                    if r.status_code <400:\n",
    "                        print(\"-> Scrape successful.\")\n",
    "                        break\n",
    "                except:\n",
    "                    print(\"-> Scrape unsuccessful. Retrying...\")\n",
    "                    time.sleep(random.randint(0,7))\n",
    "                    proxy = random.choice(proxies)\n",
    "\n",
    "            s = BeautifulSoup(r.content,'html.parser')\n",
    "            dt = s.find_all(\"li\",\"result-row\")\n",
    "\n",
    "            # SCRAPE DATA\n",
    "\n",
    "            for item in dt:\n",
    "                post_id = item.find(\"span\", \"maptag\").get(\"data-pid\")\n",
    "                url = item.find(\"a\").get(\"href\")\n",
    "                tm = item.find(\"time\").get(\"datetime\")\n",
    "                house_type = \"\"\n",
    "                address = \"\"\n",
    "                rent = \"\"\n",
    "                htb = \"\"\n",
    "                hta = \"\" \n",
    "\n",
    "                if item.find(\"span\",\"housing\") is not None:\n",
    "                    ht = item.find(\"span\",\"housing\").text.replace(\"\\n\",\"\").replace(\" \", \"\")\n",
    "\n",
    "                if item.find(\"span\", \"result-hood\") is not None:\n",
    "                    a = item.find(\"span\", \"result-hood\").text.strip()\n",
    "                    address = re.sub('[()]', '',a)\n",
    "\n",
    "                if item.find(\"span\",\"result-price\") is not None:\n",
    "                    rent = item.find(\"span\",\"result-price\").text\n",
    "\n",
    "                filewriter.writerow([post_id,tm,ht,areas[x],area_names[x],address,rent,url])\n",
    "                \n",
    "            if total<int(s.find(\"span\",\"totalcount\").text):\n",
    "                \n",
    "                link = \"https://vancouver.craigslist.org/search/apa?s=\"+str(total)+\"&availabilityMode=0&postal=\"+areas[x]+\"&search_distance=\"+str(distance)\n",
    "                total = total+120\n",
    "                continue\n",
    "                \n",
    "            break\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# with open('rental_data.csv', 'w',newline='') as csvfile:\n",
    "    \n",
    "#     filewriter = csv.writer(csvfile, delimiter=',')\n",
    "#     filewriter.writerow(['Post_ID', 'Time', 'House_type', 'Address','Rent','url'])\n",
    "#     #filewriter.writerow(['Post_ID', 'Time','lat','long', 'House_type', 'Address','Rent','url'])\n",
    "    \n",
    "#     st = np.random.randint(2, 10, size=pages)\n",
    "#     proxies = get_proxies()\n",
    "    \n",
    "#     for i in range(pages):\n",
    "        \n",
    "        \n",
    "#         if i==3:\n",
    "#             proxies = get_proxies() \n",
    "        \n",
    "#         user_agent = random.choice(useragents)\n",
    "#         headers = {'User-Agent': user_agent}\n",
    "#         proxy = random.choice(proxies)\n",
    "        \n",
    "#         time.sleep(st[i])\n",
    "    \n",
    "#         # READ FROM FILE\n",
    "    \n",
    "# #         fname = path+str(i+1)+\".txt\"\n",
    "# #         f = open(fname, 'r',encoding=\"utf8\")\n",
    "# #         s = BeautifulSoup(f,'html.parser')\n",
    "# #         dt = s.find_all(\"li\",\"result-row\")\n",
    "    \n",
    "#         # READ DIRECTLY FROM WEB PAGE\n",
    "    \n",
    "#         while True:\n",
    "#             try:\n",
    "#                 r = requests.get(link,headers=headers,proxies={\"http\": proxy, \"https\": proxy})\n",
    "#                 if r.status_code <400:\n",
    "#                     break\n",
    "#             except:\n",
    "#                 proxy = random.choice(proxies)\n",
    "            \n",
    "#         s = BeautifulSoup(r.content,'html.parser')\n",
    "#         dt = s.find_all(\"li\",\"result-row\")\n",
    "#         link = \"https://vancouver.craigslist.org/search/apa?s=\"+str(items*(i+1))\n",
    "    \n",
    "#         # SCRAPE DATA\n",
    "    \n",
    "#         for item in dt:\n",
    "#             post_id = item.find(\"span\", \"maptag\").get(\"data-pid\")\n",
    "#             url = item.find(\"a\").get(\"href\")\n",
    "#             tm = item.find(\"time\").get(\"datetime\")\n",
    "#             house_type = \"\"\n",
    "#             address = \"\"\n",
    "#             rent = \"\"\n",
    "#             htb = \"\"\n",
    "#             hta = \"\"\n",
    "            \n",
    "#             # Get Latitude and Longitude\n",
    "#             #time.sleep(random.randint(29,37))\n",
    "#             #lat,long = get_lat_lon(url,proxies,useragents)\n",
    " \n",
    "            \n",
    "#             if item.find(\"span\",\"housing\") is not None:\n",
    "#                 ht = item.find(\"span\",\"housing\").text.replace(\"\\n\",\"\").replace(\" \", \"\")\n",
    "# #                 vals = re.findall('\\d+', str(ht))\n",
    "# #                 if len(vals)==2:\n",
    "# #                     htb = vals[0]\n",
    "# #                     hta = vals[1]\n",
    "# #                 elif len(vals)==1:\n",
    "# #                     htb = vals[0]\n",
    "                \n",
    "#             if item.find(\"span\", \"result-hood\") is not None:\n",
    "#                 a = item.find(\"span\", \"result-hood\").text.strip()\n",
    "#                 address = re.sub('[()]', '',a)\n",
    "                \n",
    "#             if item.find(\"span\",\"result-price\") is not None:\n",
    "#                 rent = item.find(\"span\",\"result-price\").text\n",
    "                \n",
    "#             filewriter.writerow([post_id,tm,ht,address,rent,url])\n",
    "#             #filewriter.writerow([post_id,tm,lat,long,house_type,address,rent,url])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
